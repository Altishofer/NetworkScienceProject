{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:37:52.463811Z",
     "start_time": "2024-12-19T09:36:01.078752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "loader = DataLoader(hs_code=282520)"
   ],
   "id": "638d607edce78448",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_initialize_data took 110.1829 seconds\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T09:37:55.514487Z",
     "start_time": "2024-12-19T09:37:55.505349Z"
    }
   },
   "cell_type": "code",
   "source": "df = loader.get_data()",
   "id": "2bb957340e208283",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:22:53.227429Z",
     "start_time": "2024-12-19T10:22:53.204057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import umap\n",
    "import math\n",
    "\n",
    "#####################################\n",
    "# Helper Functions\n",
    "#####################################\n",
    "def filter_top_countries(df: pd.DataFrame, top_percent: float = 0.4):\n",
    "    \"\"\"\n",
    "    Filter the dataset to only include countries that are in the top `top_percent`\n",
    "    in terms of total trade volume.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The input dataframe with 'export_country', 'import_country', and 'v'.\n",
    "    top_percent : float\n",
    "        Fraction of countries to keep, based on total trade volume. 0.5 means top 50%.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_filtered : pd.DataFrame\n",
    "        The filtered dataframe containing only the top-percent countries.\n",
    "    \"\"\"\n",
    "    exp_sum = df.groupby('export_country')['v'].sum()\n",
    "    imp_sum = df.groupby('import_country')['v'].sum()\n",
    "\n",
    "    total_trade = exp_sum.add(imp_sum, fill_value=0).sort_values(ascending=False)\n",
    "\n",
    "    cutoff_index = int(len(total_trade)*top_percent)\n",
    "    cutoff_countries = total_trade.index[:cutoff_index]\n",
    "\n",
    "    df_filtered = df[df['export_country'].isin(cutoff_countries) & df['import_country'].isin(cutoff_countries)].copy()\n",
    "    return df_filtered\n",
    "\n",
    "#####################################\n",
    "# Step 1: Data Preparation\n",
    "#####################################\n",
    "def build_snapshots(\n",
    "    df: pd.DataFrame,\n",
    "    node_features = ['gdpcap_o','gdpcap_d','wto_o','wto_d','eu_o','eu_d','pop_o','pop_d'],\n",
    "    edge_features = [\n",
    "        'v','comlang_off','comlang_ethno','comcol','col45',\n",
    "        'comleg_pretrans','comleg_posttrans','col_dep_ever',\n",
    "        'empire','sibling_ever','scaled_sci_2021','comrelig','distw_harmonic'\n",
    "    ],\n",
    "    device: torch.device = torch.device('cpu')\n",
    "):\n",
    "    \"\"\"\n",
    "    Convert df into a list of PyG Data objects, one per snapshot (year).\n",
    "    Focuses on the specified node and edge features only, after filtering top countries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The merged dataset containing trade, gravity, and additional attributes.\n",
    "    node_features : list of str\n",
    "        Node-level features to use (aggregated by country).\n",
    "    edge_features : list of str\n",
    "        Edge-level features to use directly on edges.\n",
    "    device : torch.device, optional\n",
    "        The computation device (CPU or GPU).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    snapshots : list of Data\n",
    "        A list of PyTorch Geometric Data objects, one per year.\n",
    "    all_countries : np.ndarray\n",
    "        Array of all unique country names.\n",
    "    \"\"\"\n",
    "    df = filter_top_countries(df, top_percent=0.5)\n",
    "\n",
    "    years = sorted(df['t'].unique())\n",
    "    all_countries = np.union1d(df['export_country'].unique(), df['import_country'].unique())\n",
    "    country_to_id = {c: i for i, c in enumerate(all_countries)}\n",
    "    num_nodes = len(all_countries)\n",
    "    \n",
    "    for col in node_features+edge_features:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0.0)\n",
    "\n",
    "    df['yearly_total_v'] = df.groupby('t')['v'].transform('sum')\n",
    "    df['v'] = df['v'] / (df['yearly_total_v'] + 1e-9)\n",
    "\n",
    "    node_df = df.groupby('export_country')['gdpcap_d'].mean().rename('avg_gdpcap_exp').to_frame()\n",
    "    node_df['avg_gdpcap_imp'] = df.groupby('import_country')['gdpcap_o'].mean()\n",
    "    node_df['wto_member'] = df[['export_country','wto_o']].drop_duplicates().groupby('export_country')['wto_o'].max()\n",
    "    node_df['eu_member'] = df[['export_country','eu_o']].drop_duplicates().groupby('export_country')['eu_o'].max()\n",
    "    node_df['avg_pop'] = df.groupby('export_country')['pop_o'].mean()\n",
    "    node_df = node_df.fillna(0)\n",
    "    \n",
    "    for c in all_countries:\n",
    "        if c not in node_df.index:\n",
    "            node_df.loc[c] = [0,0,0,0,0]\n",
    "\n",
    "    node_mat = node_df[['avg_gdpcap_exp','avg_gdpcap_imp','wto_member','eu_member','avg_pop']].values\n",
    "    scaler_node = StandardScaler()\n",
    "    node_mat = scaler_node.fit_transform(node_mat)\n",
    "\n",
    "    snapshots = []\n",
    "    scaler_edge = StandardScaler()\n",
    "\n",
    "    edge_mat_all = df[edge_features].values\n",
    "    edge_mat_all = scaler_edge.fit_transform(edge_mat_all)\n",
    "\n",
    "    for year in years:\n",
    "        sub = df[df['t']==year].copy()\n",
    "        sub['src'] = sub['export_country'].map(country_to_id)\n",
    "        sub['dst'] = sub['import_country'].map(country_to_id)\n",
    "\n",
    "        sub_edge_mat = sub[edge_features].values\n",
    "        sub_edge_mat = scaler_edge.transform(sub_edge_mat)\n",
    "\n",
    "        edge_index = torch.tensor([sub['src'].values, sub['dst'].values], dtype=torch.long)\n",
    "        edge_attr = torch.tensor(sub_edge_mat, dtype=torch.float)\n",
    "        \n",
    "        x = torch.tensor(node_mat, dtype=torch.float)\n",
    "\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "        data = data.to(device)\n",
    "        snapshots.append(data)\n",
    "\n",
    "    return snapshots, all_countries\n",
    "\n",
    "#####################################\n",
    "# Dynamic GNN Autoencoder\n",
    "#####################################\n",
    "class GCNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim=64, out_channels=32, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_channels)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class DynamicGNNAutoencoder(nn.Module):\n",
    "    def __init__(self, num_nodes, in_channels, hidden_dim=64, out_channels=32, edge_dim=13, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.encoder = GCNEncoder(in_channels, hidden_dim, out_channels, dropout)\n",
    "        self.rnn = nn.GRU(out_channels, out_channels, batch_first=False)\n",
    "        self.out_dim = out_channels\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2*out_channels, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, edge_dim+1)\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        u, v = edge_index\n",
    "        z_uv = torch.cat([z[u], z[v]], dim=-1)\n",
    "        out = self.mlp(z_uv)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encoder(x, edge_index)\n",
    "        return z\n",
    "\n",
    "    def forward_timestep(self, data, h):\n",
    "        z_curr = self.encoder(data.x, data.edge_index)\n",
    "        z_curr = z_curr.unsqueeze(0)\n",
    "        if h is None:\n",
    "            h = z_curr\n",
    "        else:\n",
    "            h, _ = self.rnn(z_curr, h)\n",
    "        return h\n",
    "\n",
    "    def compute_loss(self, z, data):\n",
    "        z = z.squeeze(0)\n",
    "        pos_edges = data.edge_index\n",
    "        pos_out = self.decode(z, pos_edges)\n",
    "        pos_labels = torch.cat([torch.ones(pos_out.size(0),1,device=z.device), data.edge_attr], dim=-1)\n",
    "\n",
    "        neg_edge_index = negative_sampling(pos_edges, num_nodes=self.num_nodes, num_neg_samples=pos_edges.size(1), method='dense')\n",
    "        neg_out = self.decode(z, neg_edge_index)\n",
    "        neg_labels = torch.cat([torch.zeros(neg_out.size(0),1,device=z.device), \n",
    "                                torch.zeros((neg_out.size(0), data.edge_attr.size(1)),device=z.device)], dim=-1)\n",
    "\n",
    "        out = torch.cat([pos_out, neg_out], dim=0)\n",
    "        labels = torch.cat([pos_labels, neg_labels], dim=0)\n",
    "\n",
    "        adj_pred = out[:,0]\n",
    "        adj_true = labels[:,0]\n",
    "        adj_loss = F.binary_cross_entropy_with_logits(adj_pred, adj_true)\n",
    "\n",
    "        attr_pred = out[:,1:]\n",
    "        attr_true = labels[:,1:]\n",
    "        attr_loss = F.mse_loss(attr_pred, attr_true)\n",
    "\n",
    "        total_loss = 0.2*adj_loss + 0.8*attr_loss\n",
    "        return total_loss\n",
    "\n",
    "#####################################\n",
    "# Training the Model\n",
    "#####################################\n",
    "def train_dynamic_model(snapshots, num_nodes, in_channels, device=torch.device('cpu'), epochs=100, stability_weight=0.9):\n",
    "    edge_dim = 13\n",
    "    model = DynamicGNNAutoencoder(num_nodes, in_channels=in_channels, edge_dim=edge_dim).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        loss_total = 0\n",
    "        h = None\n",
    "        prev_z = None\n",
    "        for data in snapshots:\n",
    "            h = model.forward_timestep(data, h)\n",
    "            loss = model.compute_loss(h, data)\n",
    "\n",
    "            if prev_z is not None:\n",
    "                stability_loss = F.mse_loss(h, prev_z)\n",
    "                loss = loss + stability_weight * stability_loss\n",
    "\n",
    "            prev_z = h.clone().detach()\n",
    "            loss_total += loss\n",
    "        loss_total = loss_total / len(snapshots)\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss_total.item():.4f}\")\n",
    "    return model\n",
    "\n",
    "#####################################\n",
    "# Extract Final Embeddings & Cluster\n",
    "#####################################\n",
    "def get_final_embeddings(model, snapshots):\n",
    "    model.eval()\n",
    "    h = None\n",
    "    with torch.no_grad():\n",
    "        for data in snapshots:\n",
    "            h = model.forward_timestep(data, h)\n",
    "    return h.squeeze(0).cpu().numpy()\n",
    "\n",
    "def cluster_embeddings(emb, min_cluster_size=10):\n",
    "    reducer = umap.UMAP(random_state=42)\n",
    "    emb_2d = reducer.fit_transform(emb)\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    labels = clusterer.fit_predict(emb_2d)\n",
    "    return labels, emb_2d\n",
    "\n",
    "#####################################\n",
    "# FULL PIPELINE\n",
    "#####################################\n",
    "def run_temporal_community_detection(df: pd.DataFrame, device: torch.device = torch.device('cpu')):\n",
    "    snapshots, all_countries = build_snapshots(df, device=device)\n",
    "    in_channels = snapshots[0].x.size(1)\n",
    "    num_nodes = snapshots[0].x.size(0)\n",
    "\n",
    "    model = train_dynamic_model(snapshots, num_nodes, in_channels, device=device, epochs=200, stability_weight=0.9)\n",
    "    emb = get_final_embeddings(model, snapshots)\n",
    "    labels, emb_2d = cluster_embeddings(emb, min_cluster_size=5)\n",
    "\n",
    "    int_to_node = {i: c for i, c in enumerate(all_countries)}\n",
    "    communities = {}\n",
    "    for i, lbl in enumerate(labels):\n",
    "        if lbl not in communities:\n",
    "            communities[lbl] = []\n",
    "        communities[lbl].append(int_to_node[i])\n",
    "\n",
    "    return communities, labels, emb_2d"
   ],
   "id": "14ca8c72d786d89a",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-19T10:28:53.105259Z",
     "start_time": "2024-12-19T10:28:34.514025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "communities, labels, emb_2d = run_temporal_community_detection(df.to_pandas(), device=device)\n",
    "for c_id, members in communities.items():\n",
    "    print(f\"Community {c_id}: {members}\")\n",
    "\n",
    "total_trade = df['q'].sum()\n",
    "\n",
    "country_to_community = {}\n",
    "for c_id, members in communities.items():\n",
    "    for m in members:\n",
    "        country_to_community[m] = c_id\n",
    "\n",
    "community_trade = {c_id: 0.0 for c_id in communities.keys()}\n",
    "\n",
    "for idx, row in df.to_pandas().iterrows():\n",
    "    exp_country = row['export_country']\n",
    "    imp_country = row['import_country']\n",
    "    trade_val = row['q']\n",
    "\n",
    "    if exp_country in country_to_community and imp_country in country_to_community:\n",
    "        c_id_exp = country_to_community[exp_country]\n",
    "        c_id_imp = country_to_community[imp_country]\n",
    "        if c_id_exp == c_id_imp:\n",
    "            community_trade[c_id_exp] += trade_val\n",
    "\n",
    "for c_id, total_c_trade in community_trade.items():\n",
    "    share = (total_c_trade / total_trade) * 100 if total_trade > 0 else 0\n",
    "    print(f\"Community {c_id}: {share:.2f}% of total trade volume\")"
   ],
   "id": "34b613926bc1ca54",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.4760\n",
      "Epoch 40, Loss: 0.4660\n",
      "Epoch 60, Loss: 0.4438\n",
      "Epoch 80, Loss: 0.4574\n",
      "Epoch 100, Loss: 0.4386\n",
      "Epoch 120, Loss: 0.4370\n",
      "Epoch 140, Loss: 0.4239\n",
      "Epoch 160, Loss: 0.4130\n",
      "Epoch 180, Loss: 0.4010\n",
      "Epoch 200, Loss: 0.4050\n",
      "Community 4: ['Afghanistan', 'Azerbaijan', \"Côte d'Ivoire\", 'Guatemala', 'Iran', 'Jordan', 'Oman', 'Philippines', 'Syria', 'Trinidad and Tobago', 'Tunisia', 'United Arab Emirates', 'United Rep. of Tanzania', 'Uzbekistan', 'Venezuela', 'Viet Nam', 'Zimbabwe']\n",
      "Community -1: ['Algeria', 'Argentina', 'Austria', 'China, Hong Kong SAR', 'Croatia', 'Czechia', 'Kenya', 'Malaysia', 'Mexico', 'Nepal', 'New Zealand', 'Other Asia, nes', 'Pakistan', 'Sweden']\n",
      "Community 1: ['Australia', 'Belarus', 'Belgium', 'Brazil', 'Bulgaria', 'Denmark', 'Egypt', 'France', 'Germany', 'Greece', 'Hungary', 'Israel', 'Italy', 'Lithuania', 'Morocco', 'Netherlands', 'Poland', 'Portugal', 'Romania', 'Russian Federation', 'Slovenia', 'Spain', 'Switzerland', 'Türkiye', 'Ukraine']\n",
      "Community 2: ['Bangladesh', 'Barbados', 'Botswana', 'Brunei Darussalam', 'Cuba', 'Ecuador', 'Georgia', 'Ghana', 'Kuwait', 'Namibia', 'Qatar', 'Serbia', 'Sudan', 'Sudan (...2011)']\n",
      "Community 3: ['Bolivia (Plurinational State of)', 'Chile', 'Colombia', 'Estonia', 'Finland', 'Ireland', 'Kazakhstan', 'Latvia', 'Nigeria', 'Norway', 'Peru']\n",
      "Community 0: ['Canada', 'China', 'India', 'Indonesia', 'Japan', 'Rep. of Korea', 'Saudi Arabia', 'Singapore', 'South Africa', 'Thailand', 'USA', 'United Kingdom']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinbrundler/Documents/HS24/Network Science/UZH-Network-Science/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/kevinbrundler/Documents/HS24/Network Science/UZH-Network-Science/.venv/lib/python3.9/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/Users/kevinbrundler/Documents/HS24/Network Science/UZH-Network-Science/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/kevinbrundler/Documents/HS24/Network Science/UZH-Network-Science/.venv/lib/python3.9/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Community 4: 0.08% of total trade volume\n",
      "Community -1: 0.00% of total trade volume\n",
      "Community 1: 14.06% of total trade volume\n",
      "Community 2: 0.00% of total trade volume\n",
      "Community 3: 0.05% of total trade volume\n",
      "Community 0: 58.31% of total trade volume\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f0dbbaaaebf77e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
